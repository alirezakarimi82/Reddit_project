{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['science', 'psychology']\n",
    "abbr = [subreddit[0:3] for subreddit in subreddits]\n",
    "subreddits_abbr = dict(zip(subreddits, abbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myheader = {'User-agent': 'Ali bot 1.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping subreddit: science \n",
      "\n",
      "Scraping 500 posts ...\n",
      "Scraping 1000 posts ...\n",
      "Scraping 1500 posts ...\n",
      "Scraping 2000 posts ...\n",
      "Scraping 2500 posts ...\n",
      "Scraping 3000 posts ...\n",
      "Scraping 3500 posts ...\n",
      "Scraping 4000 posts ...\n",
      "Scraping 4500 posts ...\n",
      "Scraping 5000 posts ...\n",
      "Scraping 5500 posts ...\n",
      "Scraping 6000 posts ...\n",
      "Scraping 6500 posts ...\n",
      "Scraping 7000 posts ...\n",
      "Scraping 7500 posts ...\n",
      "Scraping 8000 posts ...\n",
      "Scraping 8500 posts ...\n",
      "Scraping 9000 posts ...\n",
      "Scraping 9500 posts ...\n",
      "Scraping 10000 posts ...\n",
      "\n",
      "Dataframe size: (10000, 4) \n",
      "\n",
      "Scraping subreddit: psychology \n",
      "\n",
      "Scraping 500 posts ...\n",
      "Scraping 1000 posts ...\n",
      "Scraping 1500 posts ...\n",
      "Scraping 2000 posts ...\n",
      "Scraping 2500 posts ...\n",
      "Scraping 3000 posts ...\n",
      "Scraping 3500 posts ...\n",
      "Scraping 4000 posts ...\n",
      "Scraping 4500 posts ...\n",
      "Scraping 5000 posts ...\n",
      "Scraping 5500 posts ...\n",
      "Scraping 6000 posts ...\n",
      "Scraping 6500 posts ...\n",
      "Scraping 7000 posts ...\n",
      "Scraping 7500 posts ...\n",
      "Scraping 8000 posts ...\n",
      "Scraping 8500 posts ...\n",
      "Scraping 9000 posts ...\n",
      "Scraping 9500 posts ...\n",
      "Scraping 10000 posts ...\n",
      "\n",
      "Dataframe size: (9991, 4) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Npost = 20\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print('Scraping subreddit:', subreddit, '\\n')\n",
    "    \n",
    "    posts = []\n",
    "    before = None\n",
    "    \n",
    "    for i in range(Npost):\n",
    "        print('Scraping {} posts ...'.format((i+1)*500))\n",
    "\n",
    "        params = {'size': 500, 'subreddit': subreddit,\n",
    "                  'before': before, 'sort' :'desc', 'sort_type': 'created_utc'}\n",
    "\n",
    "        res = requests.get(url, params=params, headers=myheader)\n",
    "        \n",
    "        if res.status_code == 200:\n",
    "            rdata = res.json()\n",
    "            posts.extend(rdata['data'])\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "            \n",
    "        before = posts[-1]['created_utc']\n",
    "        \n",
    "        time.sleep(1)\n",
    "            \n",
    "    ids = []\n",
    "    titles = []\n",
    "    texts = []\n",
    "\n",
    "    for post in posts:\n",
    "        if 'selftext' in post.keys():\n",
    "            ids.append(post['id'])\n",
    "            titles.append(post['title'])\n",
    "            texts.append(post['selftext'])\n",
    "        \n",
    "    df = pd.DataFrame({'id': ids, 'title': titles, 'text': texts, 'subreddit': subreddits_abbr[subreddit]})\n",
    "    \n",
    "    print('\\nDataframe size:', df.shape, '\\n')\n",
    "    \n",
    "    csv_file = data_folder + 'data_' + subreddits_abbr[subreddit] + '.csv'\n",
    "    \n",
    "    df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
